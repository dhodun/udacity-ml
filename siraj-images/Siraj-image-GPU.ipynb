{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_ipython().system(u'export LD_LIBRARY_PATH=/usr/local/cuda/lib64/')\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./miniconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: numpy>=1.7.1 in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: functools32 in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pytz in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: six>=1.10 in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: subprocess32 in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil in ./miniconda2/lib/python2.7/site-packages (from matplotlib)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./miniconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: olefile in ./miniconda2/lib/python2.7/site-packages (from pillow)\n",
      "Requirement already satisfied: keras in ./miniconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numpy in ./miniconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: six in ./miniconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in ./miniconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied: theano in ./miniconda2/lib/python2.7/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in ./miniconda2/lib/python2.7/site-packages (from theano->keras)\n",
      "Using TensorFlow backend.\n",
      "Requirement already satisfied: h5py in ./miniconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numpy>=1.7 in ./miniconda2/lib/python2.7/site-packages (from h5py)\n",
      "Requirement already satisfied: six in ./miniconda2/lib/python2.7/site-packages (from h5py)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "get_ipython().system(u'pip install pillow')\n",
    "get_ipython().system(u'pip install keras numpy')\n",
    "get_ipython().system(u'export LD_LIBRARY_PATH=/usr/local/cuda/lib64/')\n",
    "get_ipython().system(u'KERAS_BACKEND=tensorflow python -c \"from keras import backend\"')\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "!pip install h5py\n",
    "\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_generator = train_datagen_augmented.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first layer\n",
    "model.add(Convolution2D(32,(3,3),input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Second layer\n",
    "model.add(Convolution2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Third Layer\n",
    "model.add(Convolution2D(128,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Final Layer\n",
    "model.add(Flatten()) # flatten to single vector\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid')) #to convert to probabilities for each class\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "#binary_crossentropy great for binary classification problems\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 26s - loss: 0.7519 - acc: 0.5312 - val_loss: 0.6887 - val_acc: 0.5349\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 24s - loss: 0.7001 - acc: 0.5736 - val_loss: 0.6679 - val_acc: 0.5456\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 24s - loss: 0.6542 - acc: 0.6122 - val_loss: 0.6208 - val_acc: 0.6555\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 24s - loss: 0.6250 - acc: 0.6657 - val_loss: 0.6060 - val_acc: 0.6552\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 24s - loss: 0.5958 - acc: 0.6740 - val_loss: 0.5953 - val_acc: 0.6751\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 24s - loss: 0.5780 - acc: 0.7070 - val_loss: 0.5847 - val_acc: 0.6697\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 24s - loss: 0.5719 - acc: 0.7143 - val_loss: 0.5555 - val_acc: 0.7287\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 24s - loss: 0.5584 - acc: 0.7199 - val_loss: 0.5566 - val_acc: 0.7050\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 24s - loss: 0.5336 - acc: 0.7427 - val_loss: 0.5574 - val_acc: 0.7058\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 24s - loss: 0.5299 - acc: 0.7338 - val_loss: 0.5969 - val_acc: 0.6599\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 24s - loss: 0.5205 - acc: 0.7439 - val_loss: 0.5398 - val_acc: 0.7297\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 24s - loss: 0.5195 - acc: 0.7528 - val_loss: 0.5946 - val_acc: 0.6652\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 24s - loss: 0.4853 - acc: 0.7634 - val_loss: 0.7383 - val_acc: 0.6292\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 24s - loss: 0.4901 - acc: 0.7723 - val_loss: 0.5673 - val_acc: 0.7255\n",
      "Epoch 15/20\n",
      "55/56 [============================>.] - ETA: 0s - loss: 0.4688 - acc: 0.7869"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train\n",
    "epochs = 1\n",
    "train_samples = 1800\n",
    "validation_samples = 200\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=train_samples // batch_size, epochs=epochs,validation_data=validation_generator,\n",
    "                   validation_steps=validation_samples,\n",
    "                   callbacks = [TensorBoard(\"gs://test-ml-123/dogs-cats/logs/run_a\")])\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "model.save_weights('models/basic_cnn_30_epochs.h5')\n",
    "\n",
    "model.evaluate_generator(validation_generator, validation_samples)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Tensorboard\n",
    "Seems that TensorBoard can only easily be run in DataLab Notebooks - connect to Jupyter VM with board forwarding and then run 'tensorboard --logdir logs' and connect via client browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights('models/basic_cnn_30_epochs.h5')\n",
    "\n",
    "model.evaluate_generator(validation_generator, validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Convert This Algorithm to Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format for Cloud ML Engine\n",
    "#.\n",
    "#├── README.md\n",
    "#├── data\n",
    "#│   └── mnist.pkl\n",
    "#├── setup.py\n",
    "#└── trainer\n",
    "#    ├── __init__.py\n",
    "#    └── mnist_mlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Package Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "import argparse\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "import pickle\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# add images sync\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "\n",
    "import urllib\n",
    "\n",
    "testfile = urllib.URLopener()\n",
    "testfile.retrieve(\"https://storage.googleapis.com/dhodun1-ml/dogs-cats/data.tar.gz\", \"data.tar.gz\")\n",
    "\n",
    "import tarfile\n",
    " \n",
    "# tar file to extract\n",
    "theTarFile = 'data.tar.gz'\n",
    " \n",
    "# tar file path to extract\n",
    "extractTarPath = '.'\n",
    " \n",
    "# open the tar file\n",
    "tfile = tarfile.open(theTarFile)\n",
    " \n",
    "if tarfile.is_tarfile(theTarFile):\n",
    "    # list all contents\n",
    "    #print \"tar file contents:\"\n",
    "    #print tfile.list(verbose=False)\n",
    "    # extract all contents\n",
    "    tfile.extractall(extractTarPath)\n",
    "else:\n",
    "    print theTarFile + \" is not a tarfile.\"\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train_large'\n",
    "validation_data_dir = 'data/validation_large'\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_generator = train_datagen_augmented.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(job_dir='./tmp/example-5', **args):\n",
    "\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # first layer\n",
    "    model.add(Convolution2D(32,(3,3),input_shape=(img_width, img_height, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Second layer\n",
    "    model.add(Convolution2D(64,(3,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Third Layer\n",
    "    model.add(Convolution2D(128,(3,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # Final Layer\n",
    "    model.add(Flatten()) # flatten to single vector\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid')) #to convert to probabilities for each class\n",
    "    model.summary()\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    #binary_crossentropy great for binary classification problems\n",
    "\n",
    "    epochs = 3\n",
    "    train_samples = 1800\n",
    "    validation_samples = 200\n",
    "\n",
    "\n",
    "\n",
    "    history = model.fit_generator(train_generator, steps_per_epoch=train_samples // batch_size, epochs=epochs,validation_data=validation_generator,\n",
    "                       validation_steps=validation_samples)\n",
    "\n",
    "\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "    model.save_weights('basic_cnn_weights.h5')\n",
    "    model.save('basic_cnn_model.h5')\n",
    "    \n",
    "    # Save model.h5 on to google storage\n",
    "    with file_io.FileIO('basic_cnn_model.h5', mode='r') as input_f:\n",
    "        with file_io.FileIO(job_dir + '/models/basic_cnn_model.h5', mode='w+') as output_f:\n",
    "            output_f.write(input_f.read())\n",
    "            print ('File success written to {}/basic_cnn_model'.format(job_dir))\n",
    "            \n",
    "    # Save history for later inspection \n",
    "    history_file = open(job_dir + '/models/cnn_training_history.obj','w')\n",
    "    pickle.dump(history, history_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(model.evaluate_generator(validation_generator, validation_samples))\n",
    "\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    \n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######CLOUD ML ENGINE BOILERPLATE CODE BELOW######\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # Input Arguments\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='this model ignores this field, but it is required by gcloud',\n",
    "        default='junk'\n",
    "    )\n",
    "\n",
    "  args = parser.parse_args()\n",
    "  arguments = args.__dict__\n",
    "  output_dir = arguments.pop('output_dir')\n",
    "\n",
    "  train_model(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'keras',\n",
    "    'h5py',\n",
    "    'pillow'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Example running keras on gcloud ml-=engine.',\n",
    "    author='Drew Hodun',\n",
    "    author_email='dhodun@google.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GCS_BUCKET = 'gs://dhodun1-ml' #CHANGE THIS TO YOUR BUCKET\n",
    "PROJECT = 'dhodun1' #CHANGE THIS TO YOUR PROJECT ID\n",
    "REGION = 'us-central1' #OPTIONALLY CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=trainer \\\n",
    "   -- \\\n",
    "   --output_dir='./output' \\\n",
    "   --job-dir='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOB_NAME=dogs_cats_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "  --job-dir $GCS_BUCKET/$JOB_NAME \\\n",
    "  --module-name trainer.task \\\n",
    "  --package-path ./trainer \\\n",
    "  --region $REGION \\\n",
    "  --scale-tier=BASIC_GPU \\\n",
    "  -- \\\n",
    "  --output_dir $GCS_BUCKET/$JOB_NAME/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine jobs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine jobs stream-logs dogs_cats_170826_031620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG Plus Small Fully Connected Network on Small subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_vgg = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use VGG to pre-process Data before Transfer Learning and then Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train_large'\n",
    "validation_data_dir = 'data/validation_large'\n",
    "\n",
    "transfer_train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "transfer_validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_samples = 21000\n",
    "validation_samples = 3998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features_train = model_vgg.predict_generator(transfer_train_generator, train_samples//batch_size)\n",
    "np.save(open('models/bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "bottleneck_features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features_validation = model_vgg.predict_generator(transfer_validation_generator, validation_samples//batch_size)\n",
    "np.save(open('models/bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
    "bottleneck_features_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load(open('models/bottleneck_features_train_large.npy', 'rb'))\n",
    "train_labels = np.array([0] * (train_samples // 2) + [1] * (train_samples // 2))\n",
    "# don't have # of samples divisible by 32, so have to truncate\n",
    "train_labels = train_labels[0:20968]\n",
    "\n",
    "validation_data = np.load(open('models/bottleneck_features_validation_large.npy', 'rb'))\n",
    "validation_labels = np.array([0] * (validation_samples // 2) + [1] * (validation_samples // 2))\n",
    "# don't have # of samples divisible by 32, so have to truncate\n",
    "validation_labels = validation_labels[0:3966]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Flatten(input_shape = train_data.shape[1:]))\n",
    "model_top.add(Dense(256))\n",
    "model_top.add(Activation('relu'))\n",
    "model_top.add(Dropout(0.5))\n",
    "model_top.add(Dense(1))\n",
    "model_top.add(Activation('sigmoid'))\n",
    "model_top.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_top.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "epochs = 50\n",
    "train_samples = 1800\n",
    "validation_samples = 200\n",
    "\n",
    "\n",
    "history = model_top.fit(train_data, train_labels, batch_size=batch_size,\n",
    "                        epochs=epochs, callbacks = [TensorBoard('./logs/run_{}'.format(run))],\n",
    "                        verbose=1, validation_data=(validation_data,validation_labels))\n",
    "run = run + 1\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "model_top.save_weights('models/transfer_learning_1.h5')\n",
    "\n",
    "model_top.evaluate(validation_data, validation_labels, )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
